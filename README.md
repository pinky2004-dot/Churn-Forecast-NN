# ğŸ¦ Bank Churn Prediction using Neural Networks

## ğŸ“Œ Objective

In the banking industry, customer churn (customers leaving for competitors) is a critical challenge. To address this, this project builds a neural network-based classifier to predict whether a customer will leave the bank within the next six months. By identifying at-risk customers, banks can take proactive steps to improve retention strategies and customer satisfaction.

## ğŸ› ï¸ Techniques & Topics Covered

This project applies a comprehensive set of machine learning and deep learning techniques, including:

### 1ï¸âƒ£ Data Handling & Preprocessing
- Pandas & NumPy â€“ Data manipulation and preprocessing
- Exploratory Data Analysis (EDA) â€“ Understanding patterns and trends
- Data Cleaning â€“ Handling missing values, outliers, and inconsistencies
- Inferential Statistics â€“ Extracting meaningful insights from data
- Correlation Analysis â€“ Identifying key factors driving customer churn

### 2ï¸âƒ£ Feature Engineering & Handling Imbalance
- One-Hot Encoding â€“ Transforming categorical variables for neural networks
- Standardization & Feature Scaling â€“ Ensuring optimal model performance
- SMOTE (Synthetic Minority Over-sampling Technique) â€“ Addressing class imbalance by generating synthetic data points

### 3ï¸âƒ£ Model Development: Feedforward Neural Network (FNN)
- TensorFlow & Keras â€“ Building and training deep learning models
- Sequential Model Architecture â€“ Designing a fully connected neural network
- Activation Functions â€“ Using ReLU, Sigmoid, and Softmax for different layers
- Loss Function & Optimizer â€“
  - SGD Optimizer (initial experiment)
  - Adam Optimizer (final model selection for improved performance and convergence)
- Hidden Layer Tuning â€“ Experimenting with the number of neurons for optimal accuracy

### 4ï¸âƒ£ Model Optimization & Performance Enhancements
- Dropout Regularization â€“ Preventing overfitting by randomly deactivating neurons during training
- Batch Normalization â€“ Stabilizing learning and improving convergence
- L1 & L2 Regularization â€“ Controlling model complexity
- Hyperparameter Tuning â€“ Finding the best combination of layers, neurons, and dropout rates

## ğŸ“Š Model Evaluation & Selection

### Metrics Considered

Since churn prediction requires minimizing false negatives (i.e., customers who actually leave but are misclassified as staying), the best evaluation metric is:

  âœ… Recall â€“ Prioritizing recall ensures we capture as many churners as possible. A high recall score means fewer customers will be incorrectly classified as non-churners.

Business Translation of Model Outputs:
- True Positives (TP) â€“ Correctly predicted churners
- False Negatives (FN) â€“ Actual churners incorrectly classified as non-churners (critical to minimize)
- False Positives (FP) â€“ Customers flagged as churners but who actually stay

Since false negatives result in lost revenue and missed retention opportunities, recall is the key metric to optimize.

### Performance Comparison

I evaluated multiple model configurations by adjusting optimizers, hidden layers, and handling class imbalance with SMOTE, dropout, and batch normalization, etc.

### Final Model Selection

âŒ Models 12 - 14 were discarded due to clear overfitting issues.

âœ… Models 1, 3, and 9 performed well, with Model 9 being the best choice.

- Model 9 had the best recall (0.858) while maintaining high precision and accuracy.
- Slight overfitting was present, but the model generalized well to unseen data.

## ğŸ“Œ Practical Impact

This model is business-critical for banks as it:
- Identifies customers likely to leave, allowing proactive engagement strategies.
- Minimizes false negatives, ensuring retention efforts are directed at the right customers.
- Helps optimize marketing and loyalty programs, improving customer lifetime value.





